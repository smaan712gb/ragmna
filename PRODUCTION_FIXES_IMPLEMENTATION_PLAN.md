# Production Fixes - Implementation Plan

**Created**: November 12, 2025  
**Priority**: üö® CRITICAL - Production Quality Issues  
**Estimated Time**: 2-3 days for all fixes

---

## üìã EXECUTIVE SUMMARY

Based on the MSFT/CRWV analysis, we identified **7 critical categories** of issues preventing professional-quality reports:

| Category | Status | Impact | Priority | Est. Time |
|----------|--------|--------|----------|-----------|
| **1. Target Company Data** | ‚ùå Broken | CRITICAL | P0 | 4 hours |
| **2. Peer Company Data** | ‚ùå Broken | CRITICAL | P0 | 6 hours |
| **3. RAG Integration** | ‚ùå Not Working | HIGH | P1 | 8 hours |
| **4. Due Diligence Context** | ‚ùå No Data | HIGH | P1 | 6 hours |
| **5. Financial Normalization** | ‚ùå Failed | MEDIUM | P2 | 4 hours |
| **6. Valuation Calculations** | ‚ö†Ô∏è Invalid | HIGH | P1 | 4 hours |
| **7. Final Report** | ‚ö†Ô∏è Incomplete | MEDIUM | P2 | 2 hours |

**Total Estimated Time**: ~34 hours (2-3 days with testing)

---

## üéØ PRIORITY 0 FIXES (Must Fix First - 10 hours)

### Fix P0.1: Target Company Data Collection (4 hours)

**Problem**: CRWV showing current_price=0, market_cap=0, company_name="Unknown"

**Files to Modify**:
1. `services/data-ingestion/main.py`
2. `services/llm-orchestrator/main.py`

**Implementation Steps**:

1. **Enhance `_get_company_info()` with validation** (2 hours)
   ```python
   # Location: services/data-ingestion/main.py, line ~350
   
   def _get_company_info(self, symbol: str) -> Dict[str, Any]:
       """Get comprehensive company info with strict validation"""
       
       company_info = {'symbol': symbol}
       
       # PRIMARY SOURCE: FMP API
       fmp_api_key = os.getenv('FMP_API_KEY')
       if not fmp_api_key:
           raise ValueError("FMP_API_KEY required for data ingestion")
       
       # Get company profile
       profile_url = f"https://financialmodelingprep.com/api/v3/profile/{symbol}"
       response = requests.get(profile_url, params={'apikey': fmp_api_key}, timeout=30)
       
       if response.status_code != 200:
           raise Exception(f"FMP API failed for {symbol}: {response.status_code}")
       
       data = response.json()
       if not data or len(data) == 0:
           raise Exception(f"No data returned from FMP for {symbol}")
       
       profile = data[0]
       
       # Extract critical fields
       company_info.update({
           'company_name': profile.get('companyName', ''),
           'price': profile.get('price', 0),
           'market_cap': profile.get('mktCap', 0),
           'shares_outstanding': profile.get('mktCap', 0) / profile.get('price', 1) if profile.get('price', 0) > 0 else 0,
           'cik': profile.get('cik', ''),
           'sector': profile.get('sector', ''),
           'industry': profile.get('industry', '')
       })
       
       # FALLBACK: Use yfinance if FMP data incomplete
       if not company_info['price'] or company_info['price'] == 0:
           logger.warning(f"‚ö†Ô∏è FMP missing price for {symbol}, trying yfinance...")
           yf_rate_limiter.wait_if_needed()
           
           ticker = yf.Ticker(symbol)
           yf_info = ticker.info
           
           company_info['price'] = yf_info.get('currentPrice', yf_info.get('regularMarketPrice', 0))
           company_info['market_cap'] = yf_info.get('marketCap', 0)
           
           if not company_info['shares_outstanding']:
               company_info['shares_outstanding'] = yf_info.get('sharesOutstanding', 0)
       
       # STRICT VALIDATION
       required_fields = {
           'company_name': 'Company name',
           'price': 'Current stock price',
           'market_cap': 'Market capitalization',
           'shares_outstanding': 'Shares outstanding'
       }
       
       missing_fields = []
       for field, description in required_fields.items():
           value = company_info.get(field)
           if not value or (isinstance(value, (int, float)) and value == 0):
               missing_fields.append(f"{description} ({field})")
       
       if missing_fields:
           error_msg = f"Missing required data for {symbol}: {', '.join(missing_fields)}"
           logger.error(f"‚ùå {error_msg}")
           raise ValueError(error_msg)
       
       logger.info(f"‚úÖ Validated company info for {symbol}:")
       logger.info(f"   - Name: {company_info['company_name']}")
       logger.info(f"   - Price: ${company_info['price']:.2f}")
       logger.info(f"   - Market Cap: ${company_info['market_cap']/1e9:.2f}B")
       logger.info(f"   - Shares: {company_info['shares_outstanding']:,.0f}")
       
       return company_info
   ```

2. **Add validation in orchestrator** (1 hour)
   ```python
   # Location: services/llm-orchestrator/main.py, line ~200
   
   async def _validate_ingested_data(self, symbol: str) -> Dict[str, Any]:
       """Validate that all required data was ingested"""
       
       # Call data-ingestion service to check
       response = requests.get(
           f"{self.data_ingestion_url}/validate/{symbol}",
           headers={'X-API-Key': self.service_api_key},
           timeout=30
       )
       
       if response.status_code != 200:
           raise Exception(f"Data validation failed for {symbol}")
       
       validation = response.json()
       
       required_data = [
           'company_profile',
           'financial_statements',
           'current_price',
           'market_cap'
       ]
       
       missing = [d for d in required_data if not validation.get(d)]
       
       if missing:
           raise Exception(f"Missing required data for {symbol}: {', '.join(missing)}")
       
       logger.info(f"‚úÖ Data validation passed for {symbol}")
       return validation
   ```

3. **Test and verify** (1 hour)

**Testing Command**:
```bash
python -c "
import requests
result = requests.post(
    'http://localhost:8081/ingest/company-data',
    json={'symbol': 'CRWV', 'data_type': 'all'},
    headers={'X-API-Key': 'your-api-key'}
)
print(result.json())
"
```

---

### Fix P0.2: Peer Company Data Fetching (6 hours)

**Problem**: All 10 peer companies showing zeros for all metrics

**Files to Modify**:
1. `services/cca-valuation/main.py`

**Implementation Steps**:

1. **Rewrite `_fetch_peer_data()` method** (3 hours)
   ```python
   # Location: services/cca-valuation/main.py, line ~180
   
   def _fetch_peer_data(self, peer_symbols: List[str]) -> List[Dict[str, Any]]:
       """Fetch comprehensive peer data with validation and retry"""
       
       if not peer_symbols or len(peer_symbols) == 0:
           raise ValueError("No peer symbols provided")
       
       fmp_api_key = os.getenv('FMP_API_KEY')
       if not fmp_api_key:
           raise ValueError("FMP_API_KEY required for peer data fetching")
       
       peer_data = []
       failed_peers = []
       
       logger.info(f"üìä Fetching data for {len(peer_symbols)} peers...")
       
       for symbol in peer_symbols:
           try:
               peer_info = self._fetch_single_peer(symbol, fmp_api_key)
               
               # Validate peer has minimum required data
               if self._validate_peer_data(peer_info):
                   peer_data.append(peer_info)
                   logger.info(f"‚úÖ {symbol}: ${peer_info['market_cap']/1e9:.1f}B market cap, ${peer_info['revenue']/1e6:.0f}M revenue")
               else:
                   logger.warning(f"‚ö†Ô∏è {symbol}: Insufficient data, skipping")
                   failed_peers.append(symbol)
           
           except Exception as e:
               logger.error(f"‚ùå Failed to fetch {symbol}: {e}")
               failed_peers.append(symbol)
           
           # Rate limiting: small delay between API calls
           time.sleep(0.5)
       
       # Ensure we have minimum viable peer set
       if len(peer_data) < 3:
           raise ValueError(f"Insufficient peer data: {len(peer_data)} peers fetched (need 3+). Failed: {failed_peers}")
       
       logger.info(f"‚úÖ Successfully fetched {len(peer_data)} peer companies")
       if failed_peers:
           logger.warning(f"‚ö†Ô∏è Failed to fetch: {', '.join(failed_peers)}")
       
       return peer_data
   
   def _fetch_single_peer(self, symbol: str, fmp_api_key: str) -> Dict[str, Any]:
       """Fetch single peer with retry logic"""
       
       max_retries = 3
       
       for attempt in range(max_retries):
           try:
               # Get company profile
               profile_url = f"https://financialmodelingprep.com/api/v3/profile/{symbol}"
               response = requests.get(profile_url, params={'apikey': fmp_api_key}, timeout=30)
               
               if response.status_code == 429:  # Rate limit
                   wait_time = (2 ** attempt)  # Exponential backoff
                   logger.warning(f"Rate limited on {symbol}, waiting {wait_time}s...")
                   time.sleep(wait_time)
                   continue
               
               response.raise_for_status()
               profile_data = response.json()
               
               if not profile_data or len(profile_data) == 0:
                   raise ValueError(f"Empty response for {symbol}")
               
               profile = profile_data[0]
               
               # Get TTM income statement
               income_url = f"https://financialmodelingprep.com/api/v3/income-statement/{symbol}"
               income_response = requests.get(
                   income_url,
                   params={'apikey': fmp_api_key, 'limit': 1},
                   timeout=30
               )
               income_response.raise_for_status()
               income_data = income_response.json()
               ttm_income = income_data[0] if income_data else {}
               
               # Get balance sheet for debt/cash
               balance_url = f"https://financialmodelingprep.com/api/v3/balance-sheet-statement/{symbol}"
               balance_response = requests.get(
                   balance_url,
                   params={'apikey': fmp_api_key, 'limit': 1},
                   timeout=30
               )
               balance_response.raise_for_status()
               balance_data = balance_response.json()
               ttm_balance = balance_data[0] if balance_data else {}
               
               # Construct peer data
               peer_info = {
                   'symbol': symbol,
                   'name': profile.get('companyName', symbol),
                   'sector': profile.get('sector', ''),
                   'industry': profile.get('industry', ''),
                   'market_cap': profile.get('mktCap', 0),
                   'price': profile.get('price', 0),
                   'revenue': ttm_income.get('revenue', 0),
                   'ebitda': ttm_income.get('ebitda', 0),
                   'net_income': ttm_income.get('netIncome', 0),
                   'total_debt': ttm_balance.get('totalDebt', 0),
                   'cash': ttm_balance.get('cashAndCashEquivalents', 0),
                   'enterprise_value': 0  # Will calculate below
               }
               
               # Calculate enterprise value
               peer_info['enterprise_value'] = (
                   peer_info['market_cap'] + 
                   peer_info['total_debt'] - 
                   peer_info['cash']
               )
               
               return peer_info
           
           except Exception as e:
               if attempt == max_retries - 1:
                   raise
               logger.warning(f"Retry {attempt + 1}/{max_retries} for {symbol}: {e}")
               time.sleep(1)
       
       raise Exception(f"Failed to fetch {symbol} after {max_retries} attempts")
   
   def _validate_peer_data(self, peer_info: Dict[str, Any]) -> bool:
       """Validate peer has minimum required data"""
       
       required_fields = {
           'market_cap': 1e6,      # At least $1M
           'revenue': 1e6,         # At least $1M
           'price': 0.01           # At least $0.01
       }
       
       for field, min_value in required_fields.items():
           value = peer_info.get(field, 0)
           if value < min_value:
               logger.debug(f"Peer {peer_info['symbol']} missing {field}: {value}")
               return False
       
       return True
   ```

2. **Update peer statistics calculation** (2 hours)
   - Fix division by zero errors
   - Handle edge cases (negative values, missing data)
   - Add proper statistical functions

3. **Test with real peers** (1 hour)

**Testing Command**:
```python
# Test peer fetching
python -c "
import sys
sys.path.append('services/cca-valuation')
from main import CCAValuationService

service = CCAValuationService()
peers = service._fetch_peer_data(['FTNT', 'ADSK', 'WDAY'])
for peer in peers:
    print(f\"{peer['symbol']}: ${peer['market_cap']/1e9:.1f}B\")
"
```

---

## üéØ PRIORITY 1 FIXES (High Impact - 18 hours)

### Fix P1.1: RAG Integration & Verification (8 hours)

**Problem**: No documents being vectorized, DD working with zero context

**Files to Modify**:
1. `services/data-ingestion/main.py`
2. `services/due-diligence/main.py`

**Implementation Steps**:

1. **Add RAG operation monitoring** (3 hours)
2. **Implement wait-for-completion logic** (2 hours)
3. **Add verification before DD** (2 hours)
4. **Test RAG pipeline end-to-end** (1 hour)

### Fix P1.2: Due Diligence Context Integration (6 hours)

**Problem**: DD agents receiving zero documents, generating generic insights

**Files to Modify**:
1. `services/due-diligence/main.py`

**Implementation Steps**:

1. **Add RAG query to each DD agent** (3 hours)
2. **Implement context validation** (2 hours)
3. **Test DD with real SEC filings** (1 hour)

### Fix P1.3: Valuation Calculation Fixes (4 hours)

**Problem**: Negative P/E valuations, missing peer statistics

**Files to Modify**:
1. `services/cca-valuation/main.py`
2. `services/dcf-valuation/main.py`

**Implementation Steps**:

1. **Fix P/E calculation for negative earnings** (1 hour)
2. **Fix peer statistics calculation** (2 hours)
3. **Add valuation method selection logic** (1 hour)

---

## üéØ PRIORITY 2 FIXES (Medium Impact - 6 hours)

### Fix P2.1: Financial Normalization (4 hours)

**Files**: `services/financial-normalizer/main.py`

### Fix P2.2: Final Report Assembly (2 hours)

**Files**: `services/llm-orchestrator/main.py`

---

## üìù IMPLEMENTATION SEQUENCE

### Day 1 (8 hours)
- [x] P0.1: Target company data (4 hours)
- [x] P0.2: Peer company data (4 hours)

### Day 2 (8 hours)
- [ ] P1.1: RAG integration (8 hours)

### Day 3 (8 hours)
- [ ] P1.2: DD context (6 hours)
- [ ] P1.3: Valuations (2 hours)

### Day 4 (4 hours)
- [ ] P2.1: Financial normalization (2 hours)
- [ ] P2.2: Final report (2 hours)
- [ ] End-to-end testing (4 hours)

---

## ‚úÖ VALIDATION CHECKLIST

After implementing all fixes, validate with this checklist:

### Data Ingestion
- [ ] Target company has all required fields (name, price, market_cap, shares)
- [ ] Acquirer company has all required fields
- [ ] All 10 peers have complete data (no zeros)
- [ ] SEC filings uploaded to RAG and verified
- [ ] News articles vectorized
- [ ] Analyst reports vectorized

### RAG/Vector Database
- [ ] RAG corpus contains documents (total_rag_contexts > 0)
- [ ] Can query RAG and get relevant contexts
- [ ] All 5 DD categories have vectors (financial, legal, operational, reputational, strategic)

### Due Diligence
- [ ] documents_analyzed > 0
- [ ] DD agents have specific insights (not empty arrays)
- [ ] Risk scores based on actual analysis (not all 2/5)
- [ ] Citations to source documents

### Valuations
- [ ] Peer statistics calculated (no "Insufficient data" errors)
- [ ] CCA shows valid multiples (EV/EBITDA, EV/Revenue, P/E)
- [ ] DCF shows reasonable value
- [ ] No negative valuations

### Final Report
- [ ] Company name populated
- [ ] Current price populated
- [ ] Market cap populated
- [ ] DCF value > 0
- [ ] CCA value > 0
- [ ] Classification matches analysis ("hyper_growth" for CRWV)
- [ ] Risk level matches analysis ("high" for CRWV)

---

## üöÄ DEPLOYMENT PLAN

### Pre-Deployment
1. Run full test suite
2. Test with 3 different M&A scenarios
3. Verify all services healthy
4. Check logs for errors

### Deployment
1. Deploy services one at a time
2. Monitor logs and metrics
3. Run smoke tests after each deployment
4. Rollback plan ready

### Post-Deployment
1. Run validation checklist
2. Generate test report (MSFT/CRWV)
3. Compare with baseline
4. Document improvements

---

## üìä SUCCESS METRICS

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Target data completeness | 30% | 100% | ‚ùå |
| Peer data completeness | 0% | 100% | ‚ùå |
| RAG documents vectorized | 0 | 20+ | ‚ùå |
| DD insights generated | 0 | 50+ | ‚ùå |
| Valid valuations | 40% | 100% | ‚ö†Ô∏è |
| Report completeness | 60% | 100% | ‚ö†Ô∏è |

**Target**: All metrics at 100% or 90%+ after fixes implemented.

---

## üìû SUPPORT & ESCALATION

**If issues persist after fixes:**
1. Check logs in each service
2. Verify API keys configured
3. Test individual components in isolation
4. Review error messages in detail
5. Escalate to engineering lead

---

*This implementation plan provides a clear roadmap to address all identified issues and achieve production-quality M&A reports.*
